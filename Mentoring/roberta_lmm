import torch
from transformers import RobertaTokenizer, RobertaForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset, Dataset
from loralib import LoraConfig, inject_lora

# Step 1: Load and Prepare Dataset
# Example dataset with "input" and "output" columns
data = {
    "input": [
        "Process with risk: Data breach. What are the control descriptions?",
        "Process with risk: System failure. What are the control descriptions?"
    ],
    "output": [
        "Implement encryption and access controls to prevent unauthorized access.",
        "Ensure regular backups and system monitoring to detect failures."
    ]
}

dataset = Dataset.from_dict(data)

# Tokenizer and model initialization
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForCausalLM.from_pretrained('roberta-base')

# LoRA configuration
config = LoraConfig(
    r=4,  # Rank
    alpha=16,  # Scaling
    dropout=0.1,  # Dropout rate
)

# Inject LoRA into the model
model = inject_lora(model, config)

# Tokenization function
def tokenize_function(examples):
    inputs = tokenizer(examples['input'], padding="max_length", truncation=True, max_length=512)
    outputs = tokenizer(examples['output'], padding="max_length", truncation=True, max_length=512)
    inputs["labels"] = outputs["input_ids"]
    return inputs

# Prepare dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# Train model
trainer.train()

# Save the model
trainer.save_model("./roberta-lora-finetuned")
tokenizer.save_pretrained("./roberta-lora-finetuned")

# Load the saved model and tokenizer
loaded_model = RobertaForCausalLM.from_pretrained("./roberta-lora-finetuned")
loaded_tokenizer = RobertaTokenizer.from_pretrained("./roberta-lora-finetuned")

# Example of generating control descriptions using the loaded model
def generate_control_description(process_with_risk, model, tokenizer, temperature=0.7, top_k=50, top_p=0.95, max_length=512):
    inputs = tokenizer(process_with_risk, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        num_return_sequences=1,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Generate control descriptions using the loaded model
input_text = "Process with risk: Unauthorized access. What are the control descriptions?"
output_text = generate_control_description(input_text, loaded_model, loaded_tokenizer, temperature=0.7, top_k=50, top_p=0.95)
print(output_text)
