import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have the entire dataset stored in a pandas DataFrame called 'df'

# Convert the 'Month' column to datetime if it's not already in datetime format
df['Month'] = pd.to_datetime(df['Month'])

# Set the figure size
plt.figure(figsize=(10, 6))

# Iterate over each month and create separate histograms
for month in df['Month'].dt.strftime('%b %Y').unique():
    plt.subplot(1, len(df['Month'].dt.strftime('%b %Y').unique()), list(df['Month'].dt.strftime('%b %Y').unique()).index(month) + 1)
    sns.histplot(data=df[df['Month'].dt.strftime('%b %Y') == month], x='Dividend', kde=True)
    plt.title(month)

# Adjust the layout
plt.tight_layout()

# Show the plot
plt.show()

# Method 1: Using lambda function with apply
df['Month'] = df['Month'].apply(lambda x: calendar.month_name[x])

# Method 2: Using mapping dictionary with apply
month_mapping = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}
df['Month'] = df['Month'].apply(lambda x: month_mapping[x])


import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Step 3: Explore the data
y1 = np.array([1, 2, 3, 4, 5])  # Replace with your actual data
y2 = np.array([2, 3, 4, 5, 6])  # Replace with your actual data

# Calculate descriptive statistics
y1_mean = np.mean(y1)
y1_std = np.std(y1)

y2_mean = np.mean(y2)
y2_std = np.std(y2)

# Visualize the distributions
plt.hist(y1, bins='auto', alpha=0.7, label='y1')
plt.hist(y2, bins='auto', alpha=0.7, label='y2')
plt.legend(loc='upper right')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.title('Distributions of y1 and y2')
plt.show()

# Step 4: Compare the distributions
print('Descriptive Statistics:')
print('y1 mean:', y1_mean)
print('y1 standard deviation:', y1_std)
print('y2 mean:', y2_mean)
print('y2 standard deviation:', y2_std)

# Perform statistical tests
ks_stat, ks_p_value = stats.ks_2samp(y1, y2)
mwu_stat, mwu_p_value = stats.mannwhitneyu(y1, y2, alternative='two-sided')

print('\nStatistical Test Results:')
print('Kolmogorov-Smirnov test:')
print('KS statistic:', ks_stat)
print('p-value:', ks_p_value)

print('\nMann-Whitney U test:')
print('MWU statistic:', mwu_stat)
print('p-value:', mwu_p_value)

# Step 5: Detect drift
alpha = 0.05  # Set the significance level
if ks_p_value < alpha or mwu_p_value < alpha:
    print('\nData drift is detected from y1 to y2.')
else:
    print('\nNo significant data drift from y1 to y2.')
    
    
Certainly! Let's go through each step in more detail:

1. **Descriptive Statistics**:
   - The code calculates the mean and standard deviation for both y1 and y2 using the `numpy.mean()` and `numpy.std()` functions, respectively. These statistics provide measures of central tendency and spread of the data.
   - The mean (`y1_mean` and `y2_mean`) gives an estimate of the average value of the data, while the standard deviation (`y1_std` and `y2_std`) quantifies the dispersion or variability around the mean.

2. **Visualizing Distributions**:
   - The code uses the `matplotlib.pyplot.hist()` function to create histograms of the data.
   - A histogram is a graphical representation of the distribution of a dataset, where the data is divided into a set of bins and the height of each bin represents the frequency or count of data points falling into that bin.
   - By visualizing the histograms of y1 and y2, you can gain insights into their shapes, ranges, and potential differences in the distributions.

3. **Kolmogorov-Smirnov Test**:
   - The code performs the Kolmogorov-Smirnov (KS) test using the `scipy.stats.ks_2samp()` function.
   - The KS test is a non-parametric statistical test that compares the cumulative distributions of two samples.
   - The KS test returns a KS statistic and a p-value. The KS statistic quantifies the maximum difference between the two cumulative distribution functions, while the p-value indicates the probability of observing such a difference by chance.
   - If the p-value is below a predefined significance level (in this case, `alpha = 0.05`), it suggests that there is a significant difference between the distributions of y1 and y2.

4. **Mann-Whitney U Test**:
   - The code performs the Mann-Whitney U test using the `scipy.stats.mannwhitneyu()` function.
   - The Mann-Whitney U test, also known as the Wilcoxon rank-sum test, is a non-parametric test that compares the distributions of two independent samples.
   - The test returns a U statistic and a p-value. The U statistic represents the probability that a randomly selected observation from one group will have a higher value than a randomly selected observation from the other group. The p-value indicates the likelihood of observing such a difference in the distributions by chance.
   - Similar to the KS test, if the p-value is below the significance level (`alpha = 0.05`), it suggests a significant difference between the distributions of y1 and y2.

5. **Determining Data Drift**:
   - After performing the statistical tests, the code checks if the obtained p-values are below the significance level (`alpha = 0.05`) to determine if there is a significant data drift.
   - If either the KS test or the Mann-Whitney U test yields a p-value below the significance level, it indicates that there is a significant difference between the distributions of y1 and y2, suggesting a potential data drift from y1 to y2.
   - Conversely, if both p-values are above the significance level, it suggests that there is no significant data drift between y1 and y2.

By combining these steps, the code provides a comprehensive analysis of the data distributions, performs statistical tests to compare the distributions, and determines the presence or absence of significant data drift based on the obtained p-values.



Yes, it is possible to compute confidence intervals around the test statistic values for both the Mann-Whitney U test and the Kolmogorov-Smirnov (KS) test. However, it's important to note that the interpretation of confidence intervals for these test statistics can be a bit more complex compared to traditional parametric tests. Here's an overview of how you can compute confidence intervals for these tests:

**Mann-Whitney U Test**:
The Mann-Whitney U test does not directly provide a confidence interval for the U statistic. However, you can use bootstrapping to estimate a confidence interval. Here's a modified version of the code that includes bootstrapping to compute a confidence interval for the U statistic:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Step 3: Explore the data
y1 = np.array([1, 2, 3, 4, 5])  # Replace with your actual data
y2 = np.array([2, 3, 4, 5, 6])  # Replace with your actual data

# Calculate descriptive statistics
y1_mean = np.mean(y1)
y1_std = np.std(y1)

y2_mean = np.mean(y2)
y2_std = np.std(y2)

# Visualize the distributions
plt.hist(y1, bins='auto', alpha=0.7, label='y1')
plt.hist(y2, bins='auto', alpha=0.7, label='y2')
plt.legend(loc='upper right')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.title('Distributions of y1 and y2')
plt.show()

# Step 4: Compare the distributions
print('Descriptive Statistics:')
print('y1 mean:', y1_mean)
print('y1 standard deviation:', y1_std)
print('y2 mean:', y2_mean)
print('y2 standard deviation:', y2_std)

# Perform Mann-Whitney U test
mwu_stat, mwu_p_value = stats.mannwhitneyu(y1, y2, alternative='two-sided')

# Bootstrap for confidence interval estimation
n_bootstrap = 1000  # Number of bootstrap iterations
mwu_stats_bootstrap = []
for _ in range(n_bootstrap):
    # Resample the data with replacement
    y1_bootstrap = np.random.choice(y1, size=len(y1), replace=True)
    y2_bootstrap = np.random.choice(y2, size=len(y2), replace=True)
    
    # Calculate the test statistic for the bootstrap sample
    mwu_stat_bootstrap, _ = stats.mannwhitneyu(y1_bootstrap, y2_bootstrap, alternative='two-sided')
    mwu_stats_bootstrap.append(mwu_stat_bootstrap)

# Compute the confidence interval
ci_lower = np.percentile(mwu_stats_bootstrap, 2.5)
ci_upper = np.percentile(mwu_stats_bootstrap, 97.5)
print('\nMann-Whitney U test:')
print('MWU statistic:', mwu_stat)
print('p-value:', mwu_p_value)
print(f'95% Confidence Interval: ({ci_lower}, {ci_upper})')
```

In this code, we perform bootstrapping by resampling the data with replacement and calculating the Mann-Whitney U statistic for each bootstrap sample. We then calculate the lower and upper percentiles (2.5% and 97.5%, respectively) of the bootstrap statistics to obtain a 95% confidence interval.

**Kolmogorov-Smirnov Test**:
Similarly, the KS test does not directly provide a confidence interval for the KS statistic. However,

Certainly! If you have two independent samples and want to compare their means, you can use a t-test. The t-test assumes that the data is normally distributed and that the variances of the two groups are equal (in the case of the independent t-test). Here's an example of how you can compute a t-test and perform similar analysis as before:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Step 3: Explore the data
y1 = np.array([1, 2, 3, 4, 5])  # Replace with your actual data
y2 = np.array([2, 3, 4, 5, 6])  # Replace with your actual data

# Calculate descriptive statistics
y1_mean = np.mean(y1)
y1_std = np.std(y1)

y2_mean = np.mean(y2)
y2_std = np.std(y2)

# Visualize the distributions
plt.hist(y1, bins='auto', alpha=0.7, label='y1')
plt.hist(y2, bins='auto', alpha=0.7, label='y2')
plt.legend(loc='upper right')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.title('Distributions of y1 and y2')
plt.show()

# Step 4: Compare the distributions
print('Descriptive Statistics:')
print('y1 mean:', y1_mean)
print('y1 standard deviation:', y1_std)
print('y2 mean:', y2_mean)
print('y2 standard deviation:', y2_std)

# Perform independent t-test
t_stat, t_p_value = stats.ttest_ind(y1, y2)

# Compute the confidence interval
ci_diff = stats.t.interval(0.95, len(y1)+len(y2)-2, loc=(y1_mean-y2_mean), scale=np.sqrt(y1_std**2/len(y1) + y2_std**2/len(y2)))
ci_lower, ci_upper = ci_diff
print('\nIndependent t-test:')
print('t statistic:', t_stat)
print('p-value:', t_p_value)
print(f'95% Confidence Interval: ({ci_lower}, {ci_upper})')
```

In this code, we calculate the descriptive statistics, visualize the distributions using histograms, perform an independent t-test using the `scipy.stats.ttest_ind()` function, and compute a 95% confidence interval for the difference in means using the `scipy.stats.t.interval()` function.

It's important to note that the t-test assumes normality and equal variances, so ensure that these assumptions are met before interpreting the results. If the assumptions are violated, you may need to consider alternative non-parametric tests or transformations of the data.

Yes, the t-test is generally used for parametric distributions, specifically when the data follows a normal distribution and the variances of the compared groups are assumed to be equal (for the independent t-test). The t-test relies on the assumption of normality to make accurate inferences about the population means.

However, it's worth noting that the t-test can still be robust to violations of the normality assumption to some degree, especially when the sample sizes are large. Additionally, if the sample sizes are small and the data deviates substantially from normality, non-parametric tests or other robust statistical methods may be more appropriate.

If the normality assumption is not met or if the variances of the groups are unequal, you can consider alternative tests. For example, the Mann-Whitney U test (a non-parametric test) can be used for two independent samples when the data is not normally distributed, or Welch's t-test (a modified version of the t-test) can be used for unequal variances.

Therefore, it's important to assess the assumptions of the t-test and consider alternative tests when those assumptions are violated or in doubt.

To check for the normality of a distribution, you can use graphical methods and statistical tests. Here are a few commonly used techniques:

1. **Histogram and Q-Q Plot**:
   - Plot a histogram to visualize the distribution of the data.
   - Additionally, you can create a Q-Q (quantile-quantile) plot, which compares the quantiles of the data against the quantiles of a normal distribution.
   - If the histogram and Q-Q plot show a symmetric bell-shaped distribution and the data points align closely with the diagonal line in the Q-Q plot, it suggests normality.

2. **Shapiro-Wilk Test**:
   - The Shapiro-Wilk test is a widely used statistical test to assess the normality of a distribution.
   - The null hypothesis of the test is that the data is normally distributed. If the p-value from the test is greater than the chosen significance level (e.g., 0.05), you would fail to reject the null hypothesis, indicating the data is likely normally distributed.

   ```python
   from scipy import stats

   # y is your data
   _, p_value = stats.shapiro(y)
   if p_value > 0.05:
       print("Data is likely normally distributed")
   else:
       print("Data is not normally distributed")
   ```

3. **Kolmogorov-Smirnov Test**:
   - The Kolmogorov-Smirnov (KS) test can also be used to test for normality.
   - In this case, the test compares the empirical cumulative distribution function (CDF) of the data against the CDF of a normal distribution.
   - If the p-value from the KS test is greater than the chosen significance level (e.g., 0.05), you would fail to reject the null hypothesis of normality.

   ```python
   from scipy import stats

   # y is your data
   _, p_value = stats.kstest(y, 'norm')
   if p_value > 0.05:
       print("Data is likely normally distributed")
   else:
       print("Data is not normally distributed")
   ```

4. **Skewness and Kurtosis**:
   - Skewness measures the symmetry of a distribution, with zero indicating perfect symmetry.
   - Kurtosis measures the tailedness or peakedness of a distribution, with zero indicating a normal distribution.
   - You can calculate the skewness and kurtosis of your data and compare them to the expected values for a normal distribution (skewness=0, kurtosis=0).

   ```python
   from scipy.stats import skew, kurtosis

   # y is your data
   skewness = skew(y)
   kurt = kurtosis(y)
   if abs(skewness) < 0.5 and abs(kurt) < 0.5:
       print("Data is likely normally distributed")
   else:
       print("Data is not normally distributed")
   ```

It's important to note that these methods provide indications of normality, but they are not definitive. They should be used as exploratory tools rather than absolute determinants of normality. Additionally, it's always valuable to consider the context and domain knowledge when interpreting the results.

To create side-by-side box plots for historical data and new data, you can use the `matplotlib.pyplot.boxplot()` function. Here's an example of how you can visualize the data in side-by-side box plots:

```python
import numpy as np
import matplotlib.pyplot as plt

# Historical data and new data (replace with your actual data)
historical_data = [1, 2, 3, 4, 5]
new_data = [2, 3, 4, 5, 6]

# Combine the data
data = [historical_data, new_data]

# Create labels for the box plots
labels = ['Historical Data', 'New Data']

# Create the side-by-side box plots
plt.boxplot(data, labels=labels)

# Set labels and title
plt.xlabel('Data')
plt.ylabel('Values')
plt.title('Comparison of Historical Data and New Data')

# Show the plot
plt.show()
```

In this code, you start by defining your historical data and new data as separate lists (`historical_data` and `new_data`). Then, you combine these lists into a single list (`data`), where each element represents a dataset. 

Next, you create the side-by-side box plots using `plt.boxplot(data, labels=labels)`, where `data` is the combined dataset and `labels` are the labels for each box plot.

You can customize the labels, axes, and title of the plot as desired using the relevant `xlabel()`, `ylabel()`, and `title()` functions from Matplotlib.

Finally, calling `plt.show()` displays the side-by-side box plots.

Ensure that you have the Matplotlib library installed (`pip install matplotlib`) before running the code.


To convert a NumPy array to a list, you can use the `tolist()` method provided by NumPy. Here's an example:

```python
import numpy as np

# Example NumPy array
arr = np.array([1, 2, 3, 4, 5])

# Convert NumPy array to list
lst = arr.tolist()

print(lst)  # Output: [1, 2, 3, 4, 5]
```

In this code, the NumPy array `arr` is converted to a list using the `tolist()` method. The resulting list is assigned to the variable `lst`. Finally, `print(lst)` displays the converted list.

By calling the `tolist()` method on a NumPy array, you can easily convert the array to a Python list, which can be useful when you need to work with list-specific operations or functions.

To visualize the skewness of a dataset, you can create a histogram or a density plot. Here's an example of how you can plot a histogram with a skewness line:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import skew

# Example dataset (replace with your actual data)
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Calculate skewness
data_skewness = skew(data)

# Plot histogram with skewness line
plt.hist(data, bins='auto', alpha=0.7, label='Data')
plt.axvline(np.mean(data), color='red', linestyle='dashed', linewidth=1.5, label='Mean')
plt.axvline(np.median(data), color='green', linestyle='dashed', linewidth=1.5, label='Median')
plt.legend()
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.title('Histogram with Skewness')
plt.text(0.75, 0.9, f'Skewness: {data_skewness:.2f}', transform=plt.gca().transAxes)
plt.show()
```

In this code, the dataset is represented by the `data` NumPy array. You can replace it with your actual data. The code calculates the skewness using the `scipy.stats.skew()` function.

The histogram is plotted using `plt.hist()` with the `bins='auto'` parameter, which automatically determines the number of bins. The dashed lines represent the mean and median of the dataset. The skewness value is displayed as text in the plot using `plt.text()`.

By running this code, you can visualize the histogram of the dataset and observe the skewness indicated by the line and the skewness value displayed. A positive skewness value indicates a right-skewed distribution, while a negative value indicates a left-skewed distribution.

To compute the skewness and kurtosis values for historical data and new data, you can use the `scipy.stats.skew()` and `scipy.stats.kurtosis()` functions. Here's an example:

```python
import numpy as np
from scipy.stats import skew, kurtosis

# Historical data and new data (replace with your actual data)
historical_data = np.array([1, 2, 3, 4, 5])
new_data = np.array([2, 3, 4, 5, 6])

# Compute skewness and kurtosis
historical_skewness = skew(historical_data)
historical_kurtosis = kurtosis(historical_data)
new_skewness = skew(new_data)
new_kurtosis = kurtosis(new_data)

print('Historical Data:')
print('Skewness:', historical_skewness)
print('Kurtosis:', historical_kurtosis)

print('\nNew Data:')
print('Skewness:', new_skewness)
print('Kurtosis:', new_kurtosis)
```

In this code, you provide the historical data and new data as NumPy arrays (`historical_data` and `new_data`). Replace these arrays with your actual data.

The `skew()` function from `scipy.stats` calculates the skewness of a dataset, and the `kurtosis()` function calculates the kurtosis. The skewness and kurtosis values are assigned to variables (`historical_skewness`, `historical_kurtosis`, `new_skewness`, and `new_kurtosis`) by calling the respective functions on the data arrays.

Finally, the code prints the skewness and kurtosis values for historical data and new data.

By running this code, you can obtain the skewness and kurtosis values for the respective datasets. A skewness value close to zero suggests a relatively symmetrical distribution, while positive or negative values indicate right or left skewness, respectively. Regarding kurtosis, a value close to zero indicates a similar level of peakedness compared to a normal distribution, while positive values indicate higher peakedness (leptokurtic) and negative values indicate lower peakedness (platykurtic).



Certainly! Here's an example of Python code using PyTorch that implements Euler-Maruyama discretization and the Physics-Informed Neural Network (PINN) approach for the stochastic SIR model:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import grad
import numpy as np

# Define the SIR model equations
def SIR_equations(S, I, R, beta, gamma, N, dW):
    dS = -beta * S * I / N + np.sqrt(beta * S * I / N) * dW[0]
    dI = beta * S * I / N - gamma * I + np.sqrt(gamma * I / N) * dW[1]
    dR = gamma * I + np.sqrt(gamma * I / N) * dW[2]
    return dS, dI, dR

# Define the Physics-Informed Neural Network (PINN)
class PINN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(PINN, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.layers(x)

# Define the Euler-Maruyama discretization loss
def euler_maruyama_loss(S, I, R, beta, gamma, N, t):
    dt = t[1] - t[0]
    dW = np.random.normal(0, np.sqrt(dt), size=(3,))  # Brownian increments
    dS, dI, dR = SIR_equations(S, I, R, beta, gamma, N, dW)
    S_new = S + dt * dS
    I_new = I + dt * dI
    R_new = R + dt * dR

    return (S_new - S)**2 + (I_new - I)**2 + (R_new - R)**2

# Set up the training data
t = np.linspace(0, 1, 100)  # Time grid
S_init = 990
I_init = 10
R_init = 0
N = S_init + I_init + R_init

# Initialize the PINN model
input_dim = 1
hidden_dim = 32
output_dim = 3
model = PINN(input_dim, hidden_dim, output_dim)

# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(1000):
    # Generate a random initial condition for each epoch
    S = torch.tensor([S_init], dtype=torch.float32, requires_grad=True)
    I = torch.tensor([I_init], dtype=torch.float32, requires_grad=True)
    R = torch.tensor([R_init], dtype=torch.float32, requires_grad=True)

    optimizer.zero_grad()

    # Evaluate the PINN model
    input_data = torch.tensor(t[:, np.newaxis], dtype=torch.float32)
    output_data = model(input_data)
    S_pred, I_pred, R_pred = torch.split(output_data, 1, dim=1)

    # Compute the Euler-Maruyama loss
    loss = euler_maruyama_loss(S_pred, I_pred, R_pred, beta, gamma, N, t)

    # Backpropagation and optimization step
    loss.backward()
    optimizer.step()

    # Print the loss for monitoring
    if epoch % 100 == 0:







# Drift Framework

## Overview

Drift Framework is a Python library designed to monitor and detect data drift in production machine learning systems. Data drift refers to the change in the distribution of data over time, which can lead to degrading performance of machine learning models. This framework provides tools to preprocess data, split data, and detect drift in both features and predictions.

## Files

- `data_preprocess.py`: This file contains functions for cleaning and preprocessing the raw data. It includes functionalities like handling missing values, encoding categorical variables, scaling numerical variables, etc.

- `split.py`: This file contains functions to split the data into training and testing sets. It includes functionalities to perform random split, stratified split, and time-based split.

- `drift_framework.py`: This is the core file of the framework which contains functions to detect feature drift and prediction drift. It compares the statistical properties (like mean, variance, distribution) of the current data features and predictions with that of the training data or a reference model.

- `main.py`: This is the entry point of the framework. It orchestrates the execution of other files and provides an interface to run the entire framework.

## Installation

To install the Drift Framework, you can use `pip`:

```
pip install drift-framework
```

## Usage

1. Start by importing the necessary modules from the framework.

```python
from drift_framework import data_preprocess, split, drift_framework
```

2. Preprocess the data using the `data_preprocess` module.

```python
preprocessed_data = data_preprocess.preprocess(data)
```

3. Split the preprocessed data into training and testing sets using the `split` module.

```python
train_data, test_data = split.split_data(preprocessed_data)
```

4. Detect data drift using the `drift_framework` module.

```python
feature_drift, prediction_drift = drift_framework.detect_drift(train_data, test_data)
```

5. If `feature_drift` or `prediction_drift` is beyond a certain threshold, you may want to retrain your model or investigate further.

## Contributing

We welcome contributions to the Drift Framework! Please see our [contributing guide](CONTRIBUTING.md) for more details on how to contribute.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.



