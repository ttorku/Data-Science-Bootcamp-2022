import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have the entire dataset stored in a pandas DataFrame called 'df'

# Convert the 'Month' column to datetime if it's not already in datetime format
df['Month'] = pd.to_datetime(df['Month'])

# Set the figure size
plt.figure(figsize=(10, 6))

# Iterate over each month and create separate histograms
for month in df['Month'].dt.strftime('%b %Y').unique():
    plt.subplot(1, len(df['Month'].dt.strftime('%b %Y').unique()), list(df['Month'].dt.strftime('%b %Y').unique()).index(month) + 1)
    sns.histplot(data=df[df['Month'].dt.strftime('%b %Y') == month], x='Dividend', kde=True)
    plt.title(month)

# Adjust the layout
plt.tight_layout()

# Show the plot
plt.show()

# Method 1: Using lambda function with apply
df['Month'] = df['Month'].apply(lambda x: calendar.month_name[x])

# Method 2: Using mapping dictionary with apply
month_mapping = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}
df['Month'] = df['Month'].apply(lambda x: month_mapping[x])


import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Step 3: Explore the data
y1 = np.array([1, 2, 3, 4, 5])  # Replace with your actual data
y2 = np.array([2, 3, 4, 5, 6])  # Replace with your actual data

# Calculate descriptive statistics
y1_mean = np.mean(y1)
y1_std = np.std(y1)

y2_mean = np.mean(y2)
y2_std = np.std(y2)

# Visualize the distributions
plt.hist(y1, bins='auto', alpha=0.7, label='y1')
plt.hist(y2, bins='auto', alpha=0.7, label='y2')
plt.legend(loc='upper right')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.title('Distributions of y1 and y2')
plt.show()

# Step 4: Compare the distributions
print('Descriptive Statistics:')
print('y1 mean:', y1_mean)
print('y1 standard deviation:', y1_std)
print('y2 mean:', y2_mean)
print('y2 standard deviation:', y2_std)

# Perform statistical tests
ks_stat, ks_p_value = stats.ks_2samp(y1, y2)
mwu_stat, mwu_p_value = stats.mannwhitneyu(y1, y2, alternative='two-sided')

print('\nStatistical Test Results:')
print('Kolmogorov-Smirnov test:')
print('KS statistic:', ks_stat)
print('p-value:', ks_p_value)

print('\nMann-Whitney U test:')
print('MWU statistic:', mwu_stat)
print('p-value:', mwu_p_value)

# Step 5: Detect drift
alpha = 0.05  # Set the significance level
if ks_p_value < alpha or mwu_p_value < alpha:
    print('\nData drift is detected from y1 to y2.')
else:
    print('\nNo significant data drift from y1 to y2.')
    
    
Certainly! Let's go through each step in more detail:

1. **Descriptive Statistics**:
   - The code calculates the mean and standard deviation for both y1 and y2 using the `numpy.mean()` and `numpy.std()` functions, respectively. These statistics provide measures of central tendency and spread of the data.
   - The mean (`y1_mean` and `y2_mean`) gives an estimate of the average value of the data, while the standard deviation (`y1_std` and `y2_std`) quantifies the dispersion or variability around the mean.

2. **Visualizing Distributions**:
   - The code uses the `matplotlib.pyplot.hist()` function to create histograms of the data.
   - A histogram is a graphical representation of the distribution of a dataset, where the data is divided into a set of bins and the height of each bin represents the frequency or count of data points falling into that bin.
   - By visualizing the histograms of y1 and y2, you can gain insights into their shapes, ranges, and potential differences in the distributions.

3. **Kolmogorov-Smirnov Test**:
   - The code performs the Kolmogorov-Smirnov (KS) test using the `scipy.stats.ks_2samp()` function.
   - The KS test is a non-parametric statistical test that compares the cumulative distributions of two samples.
   - The KS test returns a KS statistic and a p-value. The KS statistic quantifies the maximum difference between the two cumulative distribution functions, while the p-value indicates the probability of observing such a difference by chance.
   - If the p-value is below a predefined significance level (in this case, `alpha = 0.05`), it suggests that there is a significant difference between the distributions of y1 and y2.

4. **Mann-Whitney U Test**:
   - The code performs the Mann-Whitney U test using the `scipy.stats.mannwhitneyu()` function.
   - The Mann-Whitney U test, also known as the Wilcoxon rank-sum test, is a non-parametric test that compares the distributions of two independent samples.
   - The test returns a U statistic and a p-value. The U statistic represents the probability that a randomly selected observation from one group will have a higher value than a randomly selected observation from the other group. The p-value indicates the likelihood of observing such a difference in the distributions by chance.
   - Similar to the KS test, if the p-value is below the significance level (`alpha = 0.05`), it suggests a significant difference between the distributions of y1 and y2.

5. **Determining Data Drift**:
   - After performing the statistical tests, the code checks if the obtained p-values are below the significance level (`alpha = 0.05`) to determine if there is a significant data drift.
   - If either the KS test or the Mann-Whitney U test yields a p-value below the significance level, it indicates that there is a significant difference between the distributions of y1 and y2, suggesting a potential data drift from y1 to y2.
   - Conversely, if both p-values are above the significance level, it suggests that there is no significant data drift between y1 and y2.

By combining these steps, the code provides a comprehensive analysis of the data distributions, performs statistical tests to compare the distributions, and determines the presence or absence of significant data drift based on the obtained p-values.
