import pandas as pd
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Step 1: Text Cleaning
def clean_text(text):
    # Remove any irrelevant characters
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    # Standardize the format
    text = text.lower().strip()
    return text

# Load the dataset
file_path = 'path_to_your_csv_file.csv'
qa_df = pd.read_csv(file_path)

# Clean the text data
qa_df['Question'] = qa_df['Question'].apply(clean_text)
qa_df['Answer'] = qa_df['Answer'].apply(clean_text)

# Step 2: Tokenization and Encoding
tokenizer = AutoTokenizer.from_pretrained("t5-small")
questions = tokenizer(qa_df['Question'].tolist(), max_length=512, truncation=True, padding="max_length", return_tensors="pt")
answers = tokenizer(qa_df['Answer'].tolist(), max_length=512, truncation=True, padding="max_length", return_tensors="pt")

# Step 3: Formatting the Input and Output
input_ids = questions['input_ids']
attention_mask = questions['attention_mask']
labels = answers['input_ids']

# Step 4: Training the Model
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=1,
)
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset={"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels},
)
trainer.train()

# Step 5: Evaluation and Testing
# Here, you should load a separate test dataset for evaluation
# For demonstration purposes, we'll use the same data as for training
results = trainer.evaluate()
print(results)
